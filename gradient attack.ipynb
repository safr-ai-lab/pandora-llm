{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "257b0a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "from transformers.optimization import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9c334eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_stats():\n",
    "    t = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    r = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    a = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"Total Memory: {t:.2f} GB\\n\"\n",
    "          f\"Reserved Memory: {r:.2f} GB ({(100*(r/t)):.2f}%)\\n\"\n",
    "          f\"Remaining Memory: {t-r:.2f} GB ({(100*(t-r)/t):.2f}%)\\n\"\n",
    "          f\"---------------------------------\\n\"\n",
    "          f\"Allocated Memory: {a:.2f} GB ({(100*(a/t)):.2f}%)\\n\"\n",
    "          f\"Percent of Reserved Allocated: {(100*(a+1e-9)/(r+1e-9)):.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ee7cd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Collate functions for loading dataset\n",
    "def collate_fn(batch):\n",
    "    tokens = [tokenizer.encode(example[\"text\"], return_tensors=\"pt\", truncation=True) for example in batch]\n",
    "    max_length = max([t.size(1) for t in tokens])\n",
    "    tokens_padded = [torch.cat([t, t.new_zeros(t.size(0), max_length - t.size(1))], dim=1) for t in tokens]\n",
    "    tokens_padded = torch.cat(tokens_padded, dim=0)\n",
    "    return tokens_padded\n",
    "\n",
    "def collate_already_encoded(batch):\n",
    "    tokens = batch\n",
    "    max_length = max([len(t['tokens']) for t in tokens])\n",
    "    tokens_padded = torch.zeros((len(tokens),max_length),dtype=torch.int)\n",
    "    for i in range(len(tokens)):\n",
    "        tokens_padded[i,:] = torch.Tensor(tokens[i]['tokens'])\n",
    "    return tokens_padded\n",
    "\n",
    "validation_dataset = load_dataset(\"the_pile_val.py\", split=\"validation\") \n",
    "mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "950add09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Model and tokenizer\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-1.4B-deduped\",\n",
    "  revision=\"step143000\",\n",
    "  cache_dir=\"./pythia-1.4B-deduped/step143000\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-1.4B-deduped\",\n",
    "  revision=\"step143000\",\n",
    "  cache_dir=\"./pythia-1.4B-deduped/step143000\",\n",
    ")\n",
    "\n",
    "model_name = \"EleutherAI/pythia-1.4B-deduped\"\n",
    "model_revision = \"step143000\"\n",
    "model_cache_dir = \"./pythia-1.4B-deduped/step143000\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.half()\n",
    "mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5284a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_perplexity(model, batch_data, bs, samplelength, device):\n",
    "    with torch.no_grad():               \n",
    "        ## Get predictions on validation data                 \n",
    "        batch_x = batch_data[:,:samplelength].to(device).detach()\n",
    "        mask  = (batch_x>0).detach()                                     \n",
    "\n",
    "        model.train(False)\n",
    "        logits_validation = model(input_ids=batch_x, attention_mask = mask)\n",
    "\n",
    "        ## Find sum of log likelihood of each sequence\n",
    "        ans = torch.zeros(bs)\n",
    "        for batch in range(bs):\n",
    "            seq_logits = torch.zeros((len(batch_x[0])))\n",
    "\n",
    "            ## Find logits of each word in the sequence\n",
    "            for idx, w in enumerate(batch_x[batch,]):\n",
    "                if batch_x[batch, idx] == 0:\n",
    "                    break\n",
    "                seq_logits[idx] = logits_validation.logits[batch,idx,w]\n",
    "            if batch_x[batch,idx] != 0:\n",
    "                idx += 1\n",
    "            ans[batch] = torch.sum(seq_logits[:idx]-torch.log(1+torch.exp(seq_logits[:idx])))/idx\n",
    "\n",
    "            ## Clean up \n",
    "            del seq_logits\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        ## Cleaning up\n",
    "        del batch_x, mask, logits_validation\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(new_model, n_new_models, noise_variance, \n",
    "                   dataloader, nbatches, bs, samplelength, device):\n",
    "  \n",
    "  new_model.eval()\n",
    "  new_model.to(device)\n",
    "  ans = torch.zeros((n_new_models+1, nbatches, bs))  \n",
    "  \n",
    "  for ind_model in range(n_new_models):\n",
    "    if ind_model == 0:\n",
    "      for batchno, batch_data in enumerate(dataloader):\n",
    "        if batchno < nbatches:\n",
    "          ans[ind_model,batchno,:] = compute_batch_perplexity(model, batch_data, bs, samplelength, device)\n",
    "        else:\n",
    "          break\n",
    "    else:\n",
    "        prevseed = torch.seed()\n",
    "        with torch.no_grad():\n",
    "          for param in new_model.parameters():\n",
    "              param.add_((torch.randn(param.size()) * noise_variance).to(device))\n",
    "        for batchno, batch_data in enumerate(dataloader):\n",
    "            if batchno < nbatches:\n",
    "              ans[ind_model,batchno,:] = compute_batch_perplexity(model, batch_data, bs, samplelength, device)\n",
    "            else:\n",
    "              break\n",
    "\n",
    "        torch.manual_seed(prevseed)\n",
    "        with torch.no_grad():\n",
    "          for param in new_model.parameters():\n",
    "              param.add_(-(torch.randn(param.size()) * noise_variance).to(device))\n",
    "\n",
    "    print(ind_model)\n",
    "    mem_stats()\n",
    "  return ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new_models = 50\n",
    "noise_variance =  0.001\n",
    "bs = 8\n",
    "samplelength = 10\n",
    "nbatches = 500\n",
    "\n",
    "training_dataset = load_dataset(\"EleutherAI/pile-deduped-pythia-random-sampled\", split=\"train\")\n",
    "validation_dataset = load_dataset(\"the_pile_val.py\", split=\"validation\") \n",
    "\n",
    "training_dataloader = DataLoader(training_dataset, batch_size = bs, collate_fn=collate_already_encoded)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size = bs, collate_fn=collate_fn)\n",
    "\n",
    "training = compare_models(model, n_new_models, noise_variance, validation_dataloader, nbatches, bs, samplelength, device)\n",
    "validation = compare_models(model, n_new_models, noise_variance, validation_dataloader, nbatches, bs, samplelength, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
