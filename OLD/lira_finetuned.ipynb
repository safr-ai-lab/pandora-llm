{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qh6Ptjan0pVL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from datasets import load_dataset\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "from transformers.optimization import AdamW\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "## Memory statistics. I had to be careful with cuda memory \n",
    "def mem_stats():\n",
    "    t = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    r = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    a = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"Total Memory: {t:.2f} GB\\n\"\n",
    "          f\"Reserved Memory: {r:.2f} GB ({(100*(r/t)):.2f}%)\\n\"\n",
    "          f\"Remaining Memory: {t-r:.2f} GB ({(100*(t-r)/t):.2f}%)\\n\"\n",
    "          f\"---------------------------------\\n\"\n",
    "          f\"Allocated Memory: {a:.2f} GB ({(100*(a/t)):.2f}%)\\n\"\n",
    "          f\"Percent of Reserved Allocated: {(100*(a+1e-9)/(r+1e-9)):.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset amazon_us_reviews (/home/ec2-user/.cache/huggingface/datasets/amazon_us_reviews/Digital_Video_Games_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31db5d39d654928a77dd5a178d1f62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"amazon_us_reviews\", \"Digital_Video_Games_v1_00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"][:len(dataset[\"train\"])//25]\n",
    "test_dataset = dataset[\"train\"][len(dataset[\"train\"])//25:2*len(dataset[\"train\"])//25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = train_dataset['review_body']\n",
    "testing_dataset = test_dataset['review_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OtxHL1OY2BBY"
   },
   "outputs": [],
   "source": [
    "N = 2\n",
    "epochs = 4\n",
    "bs = 2\n",
    "learn = 5e-5\n",
    "mod_size = \"160m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHxHr5vy2Ym-"
   },
   "source": [
    "# DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KqYIITro3gmD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory: 15.77 GB\n",
      "Reserved Memory: 0.00 GB (0.00%)\n",
      "Remaining Memory: 15.77 GB (100.00%)\n",
      "---------------------------------\n",
      "Allocated Memory: 0.00 GB (0.00%)\n",
      "Percent of Reserved Allocated: 100.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = GPTNeoXForCausalLM.from_pretrained(f\"EleutherAI/pythia-{mod_size}-deduped\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"EleutherAI/pythia-{mod_size}-deduped\")\n",
    "mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4FcxSzpK3aJH"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    global models\n",
    "    tokens = [tokenizer.encode(example, return_tensors=\"pt\", truncation=True,max_length=models[0].config.max_position_embeddings) for example in batch]\n",
    "    max_length = max([t.size(1) for t in tokens])\n",
    "    tokens_padded = [torch.cat([t, t.new_zeros(t.size(0), max_length - t.size(1))], dim=1) for t in tokens]\n",
    "    tokens_padded = torch.cat(tokens_padded, dim=0)\n",
    "    return {\n",
    "        \"input_ids\":tokens_padded,\n",
    "        \"labels\":tokens_padded,\n",
    "        \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CPNpPnzy69c6"
   },
   "outputs": [],
   "source": [
    "def arr_split(chunks, N):\n",
    "  if N % 2 != 0:\n",
    "    print(\"Need even N!\")\n",
    "    return []\n",
    "  lists = [[] for i in range(N)]\n",
    "  \n",
    "  for i in range(N):\n",
    "    for j in range(i, int(i + N/2)):\n",
    "      lists[i] += chunks[j % N]\n",
    "  \n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YM4zbaNH2ZtV"
   },
   "outputs": [],
   "source": [
    "train_chunks = [training_dataset[i * len(training_dataset)//N : (i+1) * len(training_dataset) // N] for i in range(N)]\n",
    "test_chunks = [testing_dataset[i * len(testing_dataset)//N : (i+1) * len(testing_dataset) // N] for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XU3S5IRT8Gex"
   },
   "outputs": [],
   "source": [
    "train_arr = arr_split(train_chunks, N)\n",
    "test_arr = arr_split(test_chunks, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j6xCU-MO8NVI",
    "outputId": "ee1bd4c0-363d-47e7-d89d-34ae724217ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80% overlap\n",
    "len(set(train_arr[0]).intersection(train_arr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yBastHOg3FRG"
   },
   "outputs": [],
   "source": [
    "# train_dls = [DataLoader(train_arr[i], batch_size = bs, collate_fn=collate_fn, shuffle = True) for i in range(N)]\n",
    "# test_dls = [DataLoader(test_arr[i], batch_size = bs, collate_fn=collate_fn, shuffle = True) for i in range(N)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDN4jW6w2WKy"
   },
   "source": [
    "# Shadow Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-S_422n44Eh",
    "outputId": "5bb109f5-4a89-46ac-fb5c-e9cdfd6894d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the device to use\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lIrw6Zz_2NUh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory: 15.77 GB\n",
      "Reserved Memory: 1.36 GB (8.63%)\n",
      "Remaining Memory: 14.41 GB (91.37%)\n",
      "---------------------------------\n",
      "Allocated Memory: 1.31 GB (8.33%)\n",
      "Percent of Reserved Allocated: 96.48%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [GPTNeoXForCausalLM.from_pretrained(f\"EleutherAI/pythia-{mod_size}-deduped\").to(device) for i in range(N)]\n",
    "mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import sys\n",
    "# from transformers import logging as hf_logging\n",
    "\n",
    "# hf_logging.set_verbosity_info()\n",
    "# handler = logging.StreamHandler(sys.stdout)\n",
    "# logger = hf_logging.get_logger()\n",
    "# logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainerCallback\n",
    "\n",
    "# class CustomLoggingCallback(TrainerCallback):\n",
    "#     def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "#         super().on_log(args, state, control, logs=logs, **kwargs)\n",
    "#         if state.is_local_process_zero and logs.get(\"loss\"):\n",
    "#             print(f\"Step: {state.global_step}, Loss: {logs['loss']:.6f}, Batch size: {args.train_batch_size}\")\n",
    "            \n",
    "#     def on_evaluate(self, args, state, control, logs=None, **kwargs):\n",
    "#         super().on_evaluate(args, state, control, logs=logs, **kwargs)\n",
    "#         if logs is not None and state.is_local_process_zero and logs.get(\"eval_loss\"):\n",
    "#             print(f\"Step: {state.global_step}, Evaluation Loss: {logs['eval_loss']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, val_dataset, collate_fn, batch_size, epochs):\n",
    "  model = model.to(device)\n",
    "  model.config.use_cache = False\n",
    "  training_args = TrainingArguments(output_dir=\"fine-tuning\",\n",
    "                                    do_train=True,\n",
    "                                    do_eval=True,\n",
    "                                    num_train_epochs=epochs,\n",
    "                                    per_device_train_batch_size=batch_size,\n",
    "                                    per_device_eval_batch_size=batch_size,\n",
    "                                    evaluation_strategy=\"epoch\",\n",
    "                                    # logging_strategy=\"epoch\",\n",
    "                                    # save_strategy=\"epoch\",\n",
    "                                    gradient_accumulation_steps=1,\n",
    "                                    gradient_checkpointing=False,\n",
    "                                    # fp16=True,\n",
    "                                    optim=\"adafactor\",\n",
    "                                    )\n",
    "  trainer = Trainer(model=model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=train_dataset,\n",
    "                    eval_dataset=val_dataset,\n",
    "                    tokenizer=tokenizer,\n",
    "                    data_collator=collate_fn,\n",
    "                    # callbacks = [CustomLoggingCallback()]\n",
    "                  )\n",
    "  trainer.train()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_g1_CFdEiik",
    "outputId": "64f36229-cb14-421a-902e-02791b137ca7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22391/1920141103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5816' max='5816' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5816/5816 12:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.429500</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.389200</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.767100</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.254300</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22391/1920141103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n",
      "/tmp/ipykernel_22391/1920141103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n",
      "/tmp/ipykernel_22391/1920141103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n",
      "/tmp/ipykernel_22391/1920141103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n",
      "/tmp/ipykernel_22391/1920141103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n",
      "/tmp/ipykernel_22391/1920141103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n",
      "/tmp/ipykernel_22391/1920141103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n",
      "/tmp/ipykernel_22391/1920141103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n",
      "/tmp/ipykernel_22391/1920141103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n",
      "/tmp/ipykernel_22391/1920141103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n",
      "/tmp/ipykernel_22391/1920141103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2697' max='5820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2697/5820 05:32 < 06:24, 8.12 it/s, Epoch 1.85/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.404700</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22391/1920141103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n",
      "/tmp/ipykernel_22391/1920141103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n",
      "/tmp/ipykernel_22391/1920141103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n",
      "/tmp/ipykernel_22391/1920141103.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(tokens_padded>0,dtype=int)\n"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(models):\n",
    "  print(f\"Model #{i}\")\n",
    "  m.to(device)\n",
    "  m = train(m, train_arr[i], test_arr[i], collate_fn, bs, epochs)\n",
    "  m.to('cpu')\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWUmqmguLGkf"
   },
   "source": [
    "# Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3pWx2sKb0nD"
   },
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "  model_save_name = f'model_{i}.pt'\n",
    "  path = f\"/content/gdrive/My Drive/{model_save_name}\" \n",
    "  model.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENHs6nGEQBXq"
   },
   "source": [
    "# Load Models\n",
    "(If not saving them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3y0pZOF9-ku7"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def compute_ce_loss(model, tokenizer, string):\n",
    "  input_ids = tokenizer.encode(string, return_tensors=\"pt\")\n",
    "\n",
    "  with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "  loss_fn = CrossEntropyLoss()\n",
    "  input_len = input_ids.shape[-1] - 1\n",
    "  input_ids_without_first_token = input_ids[:, 1:]\n",
    "  logits_without_last_token = logits[:, :-1, :]\n",
    "  loss = loss_fn(logits_without_last_token.view(-1, logits.size(-1)), input_ids_without_first_token.view(-1))\n",
    "  return loss\n",
    "\n",
    "def compute_confidence(ce_loss):\n",
    "  conf = math.exp(-1 * ce_loss)\n",
    "  return math.log(conf / (1-conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pmmcj2aL57WD"
   },
   "outputs": [],
   "source": [
    "# Normality Testing\n",
    "# \"empirically\" normally dist. \n",
    "\n",
    "new_model = GPTNeoXForCausalLM.from_pretrained(\"/content/gdrive/My Drive/model_10.pt\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_g2FSQp8zT-"
   },
   "outputs": [],
   "source": [
    "# Likelihood ratio attacks\n",
    "default_tokenizer = AutoTokenizer.from_pretrained( \"EleutherAI/pythia-160m-deduped\")\n",
    "string = \"Hello! I love bitcoin and artificial intelligence. Fentanyl and oxycodon, along with frequent invokations of JFK, are titivating\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGOE5mhddPiM",
    "outputId": "2018e9e7-63ea-4c31-86b8-15cc2cd63bea"
   },
   "outputs": [],
   "source": [
    "new_model.eval()\n",
    "ce_loss = compute_ce_loss(new_model, default_tokenizer, string)\n",
    "conf = compute_confidence(ce_loss)\n",
    "print(f\"Model Output: {ce_loss} {conf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM9K_Cc4Qt0s"
   },
   "source": [
    "# Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68aPyJTqa7mJ"
   },
   "outputs": [],
   "source": [
    "def get_in_models(chunk_no, N):\n",
    "  \"\"\"\n",
    "  Returns a list of models that should be used for inference for a given chunk\n",
    "  \"\"\"\n",
    "  ins = []\n",
    "  outs = []\n",
    "  for i in range(N):\n",
    "    lower = i\n",
    "    upper = int(i+N/2)\n",
    "    innit = False\n",
    "    for j in range(lower, upper):\n",
    "      if chunk_no == j % N:\n",
    "        innit = True\n",
    "    if innit:\n",
    "      ins.append(i)\n",
    "    else:\n",
    "      outs.append(i)\n",
    "  return (ins, outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ap6tQ5-RQuRI",
    "outputId": "628bfbe2-471a-4384-86f7-8afe86ba4076"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import random\n",
    "\n",
    "# Define your 30 sets of 12 numbers\n",
    "chunk = []\n",
    "chunkno = 1\n",
    "ins, outs = get_in_models(chunkno, N)\n",
    "\n",
    "inconfidences = [[] for i in range(30)]\n",
    "outconfidences = [[] for i in range(30)]\n",
    "\n",
    "# Get In Confidences\n",
    "for i, b in enumerate(random.sample(train_chunks[chunkno], 30)):\n",
    "  print(f\"{i}\")\n",
    "  for index in ins:\n",
    "    # try:\n",
    "    #   ce_loss = compute_ce_loss(models[index], default_tokenizer, b)\n",
    "    #   conf = compute_confidence(ce_loss)\n",
    "    #   inconfidences[i].append(conf)\n",
    "    #   print(f\" - {index}\")\n",
    "    # except:\n",
    "    ce_loss = compute_ce_loss(models[index], default_tokenizer, b[:2048])\n",
    "    conf = compute_confidence(ce_loss)\n",
    "    inconfidences[i].append(conf)\n",
    "    print(f\" -- {index}\")\n",
    "\n",
    "  for index in outs:\n",
    "    # try:\n",
    "    #   ce_loss = compute_ce_loss(models[index], default_tokenizer, b)\n",
    "    #   conf = compute_confidence(ce_loss)\n",
    "    #   outconfidences[i].append(conf)\n",
    "    #   print(f\" - {index}\")\n",
    "    # except:\n",
    "    ce_loss = compute_ce_loss(models[index], default_tokenizer, b[:2048])\n",
    "    conf = compute_confidence(ce_loss)\n",
    "    inconfidences[i].append(conf)\n",
    "    print(f\" -- {index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "no8vEn3D0Sbq",
    "outputId": "889e4b04-5cfc-496b-9c55-95990b64ecd1"
   },
   "outputs": [],
   "source": [
    "outconfidences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac3N68Dm1-BZ"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "inconf_copy = copy.deepcopy(inconfidences)\n",
    "outconf_copy = copy.deepcopy(outconfidences)\n",
    "\n",
    "inconf = []\n",
    "outconf = []\n",
    "\n",
    "for i in range(len(inconf_copy)):\n",
    "  if len(inconf_copy[i]) == 0 or len(outconf_copy[i]) == 0:\n",
    "    continue\n",
    "  else:\n",
    "    inconf.append(inconf_copy[i])\n",
    "    outconf.append(outconf_copy[i])\n",
    "\n",
    "inconf = inconf[:20]\n",
    "outconf = outconf[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "id": "hxPrm_YyvULB",
    "outputId": "adf09422-bc1a-42f6-e2f5-3bb743d1cc62"
   },
   "outputs": [],
   "source": [
    "# Create a figure with a specified size\n",
    "fig, axs = plt.subplots(nrows=4, ncols=5, figsize=(20, 16))\n",
    "num_bins = 20\n",
    "\n",
    "# Loop through the 30 lists in A and B\n",
    "for i in range(20):\n",
    "    # Determine the position of the subplot in the 10x3 grid\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "\n",
    "    # Create the histograms for A[i] and B[i]\n",
    "    axs[row, col].hist(inconf[i], bins=num_bins, color='red', alpha=0.5, label='IN models')\n",
    "    axs[row, col].hist(outconf[i], bins=num_bins, color='blue', alpha=0.5, label='OUT models')\n",
    "    \n",
    "    # Set the title for the subplot\n",
    "    axs[row, col].set_title(f'Graph {i+1}')\n",
    "    \n",
    "    # Add a legend to the subplot\n",
    "    axs[row, col].legend(loc='upper right')\n",
    "    axs[row, col].set_xlabel('Logit-scaled conf')\n",
    "    axs[row, col].set_ylabel('# of models')\n",
    "\n",
    "# Adjust the layout to avoid overlapping labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure as a file\n",
    "plt.savefig('histograms.png')\n",
    "\n",
    "# Display the figure\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "unlearning",
   "language": "python",
   "name": "unlearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
