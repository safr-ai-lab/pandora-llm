
# Project Description 
The goal of this project is to probe the privacy properties of LLMs, and to explore our ability to delete data from these models empirically. Given a training dataset D, and a model M, trained via a training procedure, for x in D, we want to "delete" or "unlearn" x, by producing a model M_{-x} that is close what we would obtain training on D / {x}. 


# Models & Datasets 



# Unlearning Methods Implemented 



# Membership Inference Attacks 

